import re
import pdb
from collections import Counter
import math
from joblib import Parallel, delayed
import itertools
import time
import pickle
import csv
import itertools
from nltk.corpus import brown
from random import shuffle
from nltk.util import ngrams

def main(wd_len, cap):
	'''input:
			wd_len:  integer specifying length of words to be examined
			cap: integer number of letters to remember for each word_freqs
		output:
			avg_info_by_pol: list of floats of the average entropy for each policy
			info_values_by_pol: list of lists of info values for each word in each policy
			policy_list:  list of policies which can be used to order the entries of avg_info_by_pol and info_values_by_pol'''
	#open csv document
	w = csv.writer(open("4ltr_cap2_10000wds.csv", "w"))


	#create list of words
	all_words = brown.words()
	all_words = [list(word) for word in all_words if word != ',' and word != '\'\'' and word != '``' and word != '\'' and word != '\"' and word != '.' and word != '(' and word != ')' and word != '?' and word != '!'and word != []]
	shuffle(all_words)
	all_words = [all_words[d] for d in range(len(all_words)) if len(all_words[d]) == wd_len]
	all_words = all_words[:10000]

	#make everything lowercase
	for ind1 in range(len(all_words)):
		for ind2 in range(len(all_words[ind1])):
			all_words[ind1][ind2] = all_words[ind1][ind2].lower()

	#create list of letters and make interior lists in all_words hashable
	all_letters = itertools.chain.from_iterable(all_words)
	all_letters = list(all_letters)
	all_words = [tuple(word) for word in all_words]


	n_gram_freqs = {}


	#create list of policies
	policy_list = []
	for pos1 in range(wd_len-1):
		if cap >= 2:
			for pos2 in range(1,wd_len):
				if cap >= 3:
					for pos3 in range(2,wd_len):
						if cap >= 4:
							for pos4 in range(3,wd_len):
								policy_list.append({pos1,pos2,pos3,pos4})
						else:
							policy_list.append({pos1,pos2,pos3})
				else:
					policy_list.append({pos1,pos2})
		else:
			policy_list.append({pos1})
	policy_list = list(set([tuple(pol) for pol in policy_list if len(pol) == cap]))
	print policy_list


	#create a frequency map of all combos of letters
	#combos_by_pol = []
	combo_freqs_by_pol = dict()
	combo_set = set()
	for pol in policy_list:
		pol = list(pol)
		#create a list and set of all possible letter combos for the particular positions
		all_combos = [tuple([wrd[pol[i]] for i in range(len(pol))]) for wrd in all_words]
		#combos_by_pol.append(all_combos)
		print pol
		#pdb.set_trace()
		combo_freqs_by_pol[tuple(pol)] = Counter(all_combos)
		all_combos.extend(list(combo_set))
		combo_set = set(all_combos)
		#pdb.set_trace()


	#create a frequency map of all words
	word_freqs = Counter(all_words)
	#pdb.set_trace()
	#combo_freqs_by_pol has correct values

	#find the average info content for each combo
	info_values_by_pol = []
	avg_info_by_pol = []
	for policy in policy_list:
		#pdb.set_trace()
		average_info, info_values = calc_entropy(policy, word_freqs, combo_freqs_by_pol[tuple(policy)], len(all_combos), len(all_words), all_words)
		info_values_by_pol.append(info_values)
		avg_info_by_pol.append(average_info)
		print policy, average_info
	
	#write return values to csv
	w.writerow(policy_list)
	w.writerow(info_values_by_pol)
	w.writerow(avg_info_by_pol)
	return policy_list, info_values_by_pol, avg_info_by_pol
	#pdb.set_trace()

def calc_entropy(policy, word_freqs, combo_freqs, num_combos, num_words, all_words):
	''' Calculates the average entropy when two given positions are remembered.  Sums the probability of a given word divided by the commonness of the letters at positions specified by the policy within that word, across all words.  Returns the average.
	input:
		policy: list of two positions at which letters are remembered
		word_freqs: Counter mapping each word to the number of times it appears in the sample
		combo_freqs: Counter mapping each combination of letters to the number of times it appears at the positions specified by policy_list
		num_combos: integer number of different letter combinations that occurs at the given positions within the sample
		num_words: integer number of words in the sample
		all_words: list containing tuples of letters in each word.  Words repeated in the text are repeated in all_words'''
	sum_entropy = 0
	info_values = []
	#print all_words
	for word in all_words:
		#print word
		#pdb.set_trace()
		combo = tuple([word[policy[i]] for i in range(len(policy))])
		pdb.set_trace()
		#print (float(word_freqs[word])/num_words)
		#print combo
		#print combo_freqs
		#print (float(combo_freqs[combo])/len(combo_freqs))
		#print len(combo_freqs)
		info_values.append(-math.log((float(word_freqs[word])/num_words) / (float(combo_freqs[combo])/len(combo_freqs))))
		if float(word_freqs[word])/num_words > (float(combo_freqs[combo])/len(combo_freqs)):
			pdb.set_trace()
			print combo, word, float(word_freqs[word]), float(combo_freqs[combo])
		sum_entropy += info_values[-1]
	return sum_entropy/num_words, info_values




main(4,2)
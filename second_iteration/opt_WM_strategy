import re
import pdb
from collections import Counter
import math
from joblib import Parallel, delayed
import itertools
import time
import pickle
import csv
import itertools
from nltk.corpus import brown
from random import shuffle
from nltk.util import ngrams

def main(n,wd_len, cap):
	#create list of words
	all_words = brown.words()
	all_words = [list(word) for word in all_words if word != ',' and word != '\'\'' and word != '``' and word != '\'' and word != '\"' and word != '.' and word != '(' and word != ')' and word != '?' and word != '!'and word != []]
	shuffle(all_words)
	all_words = [all_words[d] for d in range(len(all_words)) if len(all_words[d]) == wd_len]
	all_words = all_words[:20000]
	#make everything lowercase
	for ind1 in range(len(all_words)):
		for ind2 in range(len(all_words[ind1])):
			all_words[ind1][ind2] = all_words[ind1][ind2].lower()

	all_letters = itertools.chain.from_iterable(all_words)
	all_letters = list(all_letters)
	all_words = [tuple(word) for word in all_words]


	#find frequencies of n_gram occurences
	n_gram_freqs = {}
	# for ng in n_gram_list:
	# 	for i in range(len(ng)):
	# 		current = n_gram_freqs.get(str(list(ng[0:i+1])),0)
	# 		n_gram_freqs[str(list(ng[0:i+1]))] = current + 1




	#create list of policies
	policy_list = []
	for pos1 in range(wd_len-1):
		if cap >= 2:
			for pos2 in range(1,wd_len):
				if cap >= 3:
					for pos3 in range(2,wd_len):
						if cap >= 4:
							for pos4 in range(3,wd_len):
								policy_list.append({pos1,pos2,pos3,pos4})
						else:
							policy_list.append({pos1,pos2,pos3})
				else:
					policy_list.append({pos1,pos2})
		else:
			policy_list.append({pos1})
	policy_list = list(set([tuple(pol) for pol in policy_list if len(pol) == cap]))
	print policy_list


	#create a frequency map of all combos of letters
	#combos_by_pol = []
	combo_freqs_by_pol = dict()
	combo_set = set()
	for pol in policy_list:
		pol = list(pol)
		#create a list and set of all possible letter combos for the particular positions
		all_combos = [tuple([wrd[pol[i]] for i in range(len(pol))]) for wrd in all_words]
		#combos_by_pol.append(all_combos)
		print pol
		#pdb.set_trace()
		all_combos.extend(list(combo_set))
		combo_set = set(all_combos)
		#pdb.set_trace()
		combo_freqs_by_pol[tuple(pol)] = Counter(all_combos)


	#create a frequency map of all words
	word_freqs = Counter(all_words)

	#find the average info content for each combo
	for policy in policy_list:
		#pdb.set_trace()
		average_info = calc_entropy(policy, word_freqs, combo_freqs_by_pol[tuple(pol)], len(all_combos), len(all_words), all_words)
		print policy, average_info
	#pdb.set_trace()

def calc_entropy(policy, word_freqs, combo_freqs, num_combos, num_words, all_words):
	''' Calculates the average entropy when two given positions are remembered.  Sums the probability of a given word divided by the commonness of the letters at positions specified by the policy within that word, across all words.  Returns the average.
	input:
		policy: list of two positions at which letters are remembered
		word_freqs: Counter mapping each word to the number of times it appears in the sample
		combo_freqs: Counter mapping each combination of letters to the number of times it appears at the positions specified by policy_list
		num_combos: integer number of different letter combinations that occurs at the given positions within the sample
		num_words: integer number of words in the sample
		all_words: list containing tuples of letters in each word.  Words repeated in the text are repeated in all_words'''
	sum_entropy = 0
	for word in all_words:
		combo = tuple([word[policy[i]] for i in range(len(policy))])
		#pdb.set_trace()
		sum_entropy += math.log((float(word_freqs[word])/num_words) / (float(combo_freqs[combo])/len(combo_freqs)))
	return sum_entropy/num_words



	#pdb.set_trace()
	#iterate through policies
	probs = {}
	highest_info = 0
	for policy in policy_list:
		count = 0
		sum_info = 0
		avg_info = 0
		sum_wd_info = 0
		for word_idx in range(len(all_words)):
			#pick out the words that match the positions specified by policy
			#check the uniqueness
			#take average uniqueness
			sum_wd_info = 0
			count +=1
			for letter_pos in policy:
				if n_grams_by_words[word_idx][letter_pos] in probs:
					sum_wd_info += -math.log(probs[str(list(n_grams_by_words[word_idx][letter_pos]))])
					#print letter_pos
					#print str(list(n_grams_by_words[word_idx][letter_pos]))
				else:
					probs[str(list(n_grams_by_words[word_idx][letter_pos]))] = (float(n_gram_freqs[str(list(n_grams_by_words[word_idx][letter_pos]))])/short_gram_freqs[str(n_grams_by_words[word_idx][letter_pos][0:-1])]) / len(all_letters)
					sum_wd_info += -math.log(probs[str(list(n_grams_by_words[word_idx][letter_pos]))])
					#print letter_pos
					#print str(list(n_grams_by_words[word_idx][letter_pos]))
			#pdb.set_trace()
			sum_info += sum_wd_info/cap
			#print sum_info
		avg_info = sum_info/count
		#pdb.set_trace()
		print avg_info 
		if avg_info > highest_info:
			highest_info = avg_info
			best_policy = list(policy)
		print best_policy, highest_info
		print policy, avg_info
	return best_policy, highest_info




def divide_into_grams(n,all_words):
	'''input:
			n is the number of words in a n_gram
			all_words is a list of all of the words in the corpus
		output:
			n_gram_list is the list of all n-grams in the corpus'''
	#split into sentences, then make lists of n-grams from the sentences, it should loop through len(sentences)-n, if that value is negative, skip
	n_grams_by_words = []
	n_gram_list = []
	gram_dict = dict()

	timea = time.time()
	n_grams_by_words = Parallel(n_jobs=2)(delayed(ngrams)(word, n) for word in all_words)
	#collect the words with fewer than n-1 words preceding them
	# n_grams_by_sentences = [n_grams_by_sentences[i] for i in range(len(n_grams_by_sentences)) if n_grams_by_sentences[i]]
	# split_sentences = [sentence.split() for sentence in sentence_list]
	# split_sentences = [split_sentences[k] for k in range(len(split_sentences)) if split_sentences[k]]

	#removes empty lists
	for word_grams in n_grams_by_words:
		word_grams = [x for x in word_grams if x]

	#add grams shorter than n
	for i in range(len(all_words)):
		if str(all_words[i]) in gram_dict:
			n_grams_by_words[i] = gram_dict[str(all_words[i])]
		else:
			for j in range(n-2,-1, -1):
				n_grams_by_words[i] = [tuple(all_words[i][0:j+1])] + n_grams_by_words[i]
			gram_dict[str(all_words[i])] = n_grams_by_words[i]
			#print j, n_grams_by_words[i]

	print time.time() - timea
	n_gram_list = [item for sublist in n_grams_by_words for item in sublist]
	#pdb.set_trace()




	return n_gram_list, n_grams_by_words
main(1,4,2)
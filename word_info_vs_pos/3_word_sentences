
import re
corpus = "wikipedia2text-extracted.txt"
import pdb
from collections import Counter
import math
from joblib import Parallel, delayed
import itertools
import time
import pickle
import csv
from nltk.corpus import brown
from random import shuffle
from nltk.util import ngrams

def main(n):
	'''input: 
			n: the number of loop through every first word, determine how surprising each set of words is.  Then determine the probability of surprise by position'''
	start_time = time.time()
	print start_time

	w = csv.writer(open("pos_info_3_word_sents.csv", "w"))


	#sentence_list, all_words = divide_into_sentences()
	#pdb.set_trace()
	sentence_list = list(brown.sents())
	all_words = brown.words()
	sentence_list = [list(sent) for sent in sentence_list]

	for i in range(len(sentence_list)):
		sentence_list[i] = filter(lambda a: a != ',' and a != '``' and a != '\'\'' and a != '.' and a != '(' and a != ')' and a != '?' and a != '!', sentence_list[i]) 
	#pdb.set_trace()

	sentence_list = [sent for sent in sentence_list if len(sent) == 3]


	time1 = time.time() - start_time
	print str(time1) + " time to divide into sentences"
	
	n_gram_list, n_grams_by_sentences = divide_into_grams(n,sentence_list)

	time2 = time.time() - time1
	print str(time2) + " time to divide into grams"




	#Find maximum sentence length
	maxm = len(sentence_list[0])
	for i in range(len(sentence_list)):
		if len(sentence_list[i]) > maxm:
			maxm = len(sentence_list[i])
		
	avg_info_start = []
	avg_info_end = []
	y_start = []
	y_end = []
	x = []
	info_list = []
	all_probs = dict()
	#pdb.set_trace()

	#
	for word_pos in range(maxm):
		avg_info_start, avg_info_end, all_probs, info_list = calc_avg_surprise(n, sentence_list, word_pos, n_grams_by_sentences, n_gram_list, all_probs, all_words)
		y_start.append(avg_info_start)
		y_end.append(avg_info_end)
		x.append(word_pos)
		full_info_list.append(info_list)
	w.writerow(y_end)
	w.writerow(y_start)
	w.writerow(x)
	w.writerow(full_info_list)
	return x, y_start, y_end, full_info_list

	#surprise_values = 


def divide_into_sentences():
	'''produces a list of sentences, exluding parenthetical statements and quotations, from a corpus'''
	crs = open(corpus,'r')
	sentence_list = []
	all_words = []

	for raw in crs:
		raw = raw.lower()
		sentence_list.extend(re.split('[!?.]', raw.strip()))
	#sentence_list = [elm[:-1] for elm in sentence_list]
	#print sentence_list[:10]
	#sentence_list = sentence_list[:10]
	for ind in range(len(sentence_list)):
		sentence_list[ind].lower()
		sentence_list[ind] = re.sub(r'\([^)]*\)','',sentence_list[ind])
		sentence_list[ind] = re.sub(r'\"[^"]*\"','',sentence_list[ind])
		all_words.extend(sentence_list[ind].split())
	#print "length of all_words " + str(len(all_words))
	#print sentence_list
#	pdb.set_trace()

	#pdb.set_trace()
	sentence_list = [sentence for sentence in sentence_list if sentence != '' and sentence != ' ' and len(sentence.split()) > 1]
	shuffle(sentence_list)
	#print sentence_list
	sentence_list  = [sen for sen in sentence_list if len(sen.split()) == 3]
	sentence_list =  sentence_list[:30000]
	return sentence_list, all_words



def divide_into_grams(n,sentence_list):
	'''input:
			n is the number of words in a n_gram
			sentence_list is a list of all of the sentences in the corpus
		output:
			n_gram_list is the list of all n-grams in the corpus'''
	#split into sentences, then make lists of n-grams from the sentences, it should loop through len(sentences)-n, if that value is negative, skip
	n_grams_by_sentences = []
	n_gram_list = []


	timea = time.time()
	n_grams_by_sentences = Parallel(n_jobs=2)(delayed(ngrams)(sentence, n) for sentence in sentence_list)
	#collect the words with fewer than n-1 words preceding them
	# n_grams_by_sentences = [n_grams_by_sentences[i] for i in range(len(n_grams_by_sentences)) if n_grams_by_sentences[i]]
	# split_sentences = [sentence.split() for sentence in sentence_list]
	# split_sentences = [split_sentences[k] for k in range(len(split_sentences)) if split_sentences[k]]

	for j in range(n-1):
		for i in range(len(sentence_list)):
			n_grams_by_sentences[i] = [sentence_list[i][j:n-1]] + n_grams_by_sentences[i]
		#pdb.set_trace()
		#n_gram_list = [sentence.split()[j] for sentence in sentence_list]
	print time.time() - timea
	#pdb.set_trace()
	n_gram_list = [item for sublist in n_grams_by_sentences for item in sublist]




	return n_gram_list, n_grams_by_sentences

def generate_prob_list(n, n_gram_list, all_words, n_grams_by_sentences):
	'''Generates a list of probabilities of each word given the preceding n-1 words. A list of lists for each sentence. Each element in each inner list corresponds to the probability of that word given the preceding words.
	Generates a dictionary in which each ngram maps to a probability'''
	#counter cannot work with lists of lists, so I make lists strings
	new_n_gram_list = [str(elm) for elm in n_gram_list]
	n_gram_freqs = Counter(new_n_gram_list)
	word_freqs = Counter(all_words)
	n_grams_unique = list(n_gram_list)
	n_grams_unique.sort()
	n_grams_unique = list(gram for gram,_ in itertools.groupby(n_gram_list))


		#list(frozenset(n_gram_list))

	for ngram in n_grams_unique:
		if len(ngram) < n:
			#print n_gram, str(n_gram), n_gram_freqs[str(n_gram)]
			#if you have a short ngram, count grams that include it at the beginning as adding to its frequency
			n_gram_freqs[str(ngram)] = n_gram_freqs[str(ngram)] + sum([1 for ng in n_gram_list if ngram == ng[0:len(ngram)] and len(ng) > len(ngram)])

	all_probs = []
	for n_grams in n_grams_by_sentences:
		prob_list = []
		for n_gram in n_grams:
			prob_list.append(float(n_gram_freqs[str(n_gram)])/word_freqs[n_gram[0]])
		all_probs.append(prob_list)


	return all_probs


def calc_avg_surprise(n, sentence_list, word_pos, n_grams_by_sentences, n_gram_list, probs, all_words):
	'''input:
			n: number of words in a gram.  
			sentence_list: a list of all the sentences in the corpus.  Within each list is a list of words in that sentence.
			word_pos:  specifies which words are being examined. The number of words into the sentence that the word is.  In the sentence, "The dog ran so fast.," if word_pos equals 2, We are looking at "dog" and "so".
			probs:  a dictionary containing the probabilities of a word given the preceding word
			n_grams_by_sentences is a list with an element for each sentence.  Each element is a list of the n-grams in that sentence.
		
		Runtime:
			sum_start: the sum of information values for all words at word_pos in each sentence.
			sum_end: the sum of information values for all words at -word_pos in each sentence.
	'''
	print 'calculating probability of surprise given position'
	count_end = 0
	count_start = 0
	cumulative_sum = 0




	#Initialize variables
	sum_start = 0
	sum_end = 0
	probs = dict()
	new_n_gram_list = [str(elm) for elm in n_gram_list]
	word_freqs = Counter(all_words)
	n_grams_unique = list(n_gram_list)
	n_grams_unique.sort()
	n_grams_unique = list(gram for gram,_ in itertools.groupby(n_gram_list))
	#print time.time()

	#Calculate n-gram frequencies
	#if you have a short ngram, count grams that include it at the beginning as adding to its frequency
	n_gram_freqs = {}
	for ng in n_gram_list:
		for i in range(len(ng)):
			current = n_gram_freqs.get(str(list(ng[0:i+1])),0)
			n_gram_freqs[str(list(ng[0:i+1]))] = current + 1

#	n_gram_freqs[str(ngram)] = [n_gram_freqs[str(ngram)] + sum([1 for ng in n_gram_list if ngram == ng[0:len(ngram)] and len(ng) > len(ngram)]) for ngram in n_grams_unique if len(ngram) < n]
	print time.time()
	
	#Compute average information values for words at word_pos from the beginning and end of the sentence
	#loops through each sentence, collecting information values for words at word_pos from the beginning and end of the sentence
	#when it calculate information value, it checks whether the probability of seeing that word given the preceding word is already known
	#if it is known, it uses that probability to calculate the average surprise value.  Otherwise it calculates that probability, stores it in a dictionary, and uses it to calculate the surprise value.
	for i in range(len(sentence_list)):
		if len(sentence_list[i]) - 1 >= word_pos:
			if str(list(n_grams_by_sentences[i][word_pos])) in probs:
				#if str(n_grams_by_sentences[i][-word_pos]) == '(\'"covert\', \'propaganda\')':
					#print word_pos, i
					#print n_grams_by_sentences[i][word_pos]
					#pdb.set_trace()
				infos_start.append(-math.log(probs[str(list(n_grams_by_sentences[i][word_pos]))])
				sum_start += -math.log(probs[str(list(n_grams_by_sentences[i][word_pos]))])
				count_start += 1
			else:
				#pdb.set_trace()
				probs[str(list(n_grams_by_sentences[i][word_pos]))] = float(n_gram_freqs[str(list(n_grams_by_sentences[i][word_pos]))])/word_freqs[n_grams_by_sentences[i][word_pos][-1]]#the las word in the gram
				infos_start.append(-math.log(probs[str(list(n_grams_by_sentences[i][word_pos]))])]
				sum_start += -math.log(probs[str(list(n_grams_by_sentences[i][word_pos]))])
				count_start += 1

			if str(list(n_grams_by_sentences[i][-word_pos])) in probs:
				sum_end += -math.log(probs[str(list(n_grams_by_sentences[i][-word_pos]))])
				count_end += 1
			else:
				probs[str(list(n_grams_by_sentences[i][-word_pos]))] = float(n_gram_freqs[str(list(n_grams_by_sentences[i][word_pos]))])/word_freqs[n_grams_by_sentences[i][word_pos][-1]]#the las word in the gram
				sum_end += -math.log(probs[str(list(n_grams_by_sentences[i][-word_pos]))])
				count_end += 1


	return sum_start/count_start, sum_end/count_end, probs, infos start





main(2)